import csv
import os
import time
import re
import sys
import shutil

import json
from playwright.sync_api import sync_playwright
from datetime import datetime


def clean_text(text):
    text = text.replace("\n", " ").replace("\r", " ").replace("\t", " ")
    text = re.sub(' +', ' ', text)  # ç§»é™¤å¤šé¤˜ç©ºæ ¼
    return text.strip()

def extract_article_content(page):
    title = page.title().strip()
    texts = page.locator("div.article_body").all_inner_texts()
    full_text = "\n".join(texts).strip()
    return title, full_text
def files_are_equal(file1, file2):
    if not os.path.exists(file2):
        return False
    with open(file1, "r", encoding="utf-8-sig") as f1, open(file2, "r", encoding="utf-8-sig") as f2:
        return f1.read() == f2.read()

def run(output_file, previous_file):
    existing_urls = set()
    if os.path.exists(previous_file):
        with open(previous_file, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            for row in reader:
                existing_urls.add(row["URL"])
        print(f"ğŸ§  å·²çˆ¬é {len(existing_urls)} ç¯‡æ–‡ç« ï¼Œå°‡è·³éé€™äº› URL")
    else:
        print("ğŸ†• æ²’æœ‰ previous_fileï¼Œå°‡å¾é›¶é–‹å§‹çˆ¬")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        BASE_URL = "https://slack.com/intl/zh-tw/help"
        page.goto(BASE_URL)

        category_links = page.locator("a[href*='/help/categories/']").evaluate_all(
            "links => Array.from(links, a => a.href)"
        )
        category_links = list(set(category_links))
        print(f"âœ… ç™¼ç¾ {len(category_links)} å€‹åˆ†é¡")

        article_urls = []
        article_category_map = {}

        for cat_url in category_links:
            page.goto(cat_url)
            links = page.locator("a[href*='/help/articles/']").evaluate_all(
                "links => Array.from(links, a => a.href)"
            )
            for link in links:
                article_urls.append(link)
                article_category_map[link] = cat_url
            time.sleep(1)

        article_urls = list(set(article_urls))
        print(f"ğŸ“ å…±ç™¼ç¾ {len(article_urls)} ç¯‡æ–‡ç« ")

        results = []
        for idx, url in enumerate(article_urls):
            if url in existing_urls:
                print(f"â© è·³éå·²çˆ¬éæ–‡ç«  ({idx+1}/{len(article_urls)}): {url}")
                continue

            print(f"ğŸ” æ­£åœ¨è™•ç†ç¬¬ {idx+1}/{len(article_urls)} ç¯‡: {url}")
            try:
                page.goto(url, timeout=60000)
                title, text = extract_article_content(page)
                category = article_category_map.get(url, "æœªçŸ¥åˆ†é¡")
                if idx == 0:
                    title += " æ¸¬è©¦æ”¹1"

                results.append({
                    "Title": clean_text(title),
                    "Text": clean_text(text),
                    "Category": clean_text(category),
                    "URL": url
                })
            except Exception as e:
                print(f"âš ï¸ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                continue

        browser.close()

        # å¯«å…¥ output_fileï¼ˆæœ¬æ¬¡æ–°çµæœï¼‰
        with open(output_file, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=["Title", "Text", "Category", "URL"])
            writer.writeheader()
            if results:
                for row in results:
                    writer.writerow(row)
            else:
                # æ²’æ–°è³‡æ–™ä¹Ÿå…ˆç…§åŸæ¨£å¯«å…¥ç©ºæª”ï¼ˆä¹‹å¾Œå†è¦†è“‹ç‚º dummyï¼‰
                pass

        # æª¢æŸ¥æ˜¯å¦èˆ‡ previous_file ç›¸åŒ
        if files_are_equal(output_file, previous_file):
            print("ğŸ“­ è³‡æ–™ç›¸åŒï¼Œä¸æ›´æ–° previous_fileï¼Œè¦†è“‹ output_file ç‚º dummy")
            with open(output_file, "w", newline="", encoding="utf-8-sig") as f:
                writer = csv.DictWriter(f, fieldnames=["Title", "Text", "Category", "URL"])
                writer.writeheader()
                writer.writerow({
                    "Title": 1,
                    "Text": 1,
                    "Category": 1,
                    "URL": f"empty-{datetime.now().isoformat()}"
                })
            return output_file
        else:
            print("âœ… è³‡æ–™æœ‰æ›´æ–°ï¼Œå·²å¯«å…¥ previous_file")
            shutil.copy(output_file, previous_file)
            return output_file
if __name__ == "__main__":
    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)

    output_file = os.path.join(output_dir, "slack_articles_with_category.csv")   # æ–°çˆ¬çš„
    previous_file = os.path.join(output_dir, "slack_articles_previous.csv")      # ä¸Šä¸€æ¬¡çš„

    result_path = run(output_file, previous_file)
